{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시간 단축 only\n",
    "\n",
    "# KB에 없는 negative triple 생성하여 symbolic Unification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Theorem Prover using pandas and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "- Load Files \n",
    "- Define Functions \n",
    "- Generate Meta Tables\n",
    "- Run Symbolic Unification and generate batch \n",
    "\n",
    "## 2. NTP Model Training with PyTorch\n",
    "- Define Model Structure using PyTorch\n",
    "- Define Foward Function \n",
    "- Training Model\n",
    "\n",
    "## 3. Extract Rules from Trained Embedding Vectors\n",
    "- Matching Rule templates with Embedding vectors \n",
    "- Extract Induced Rules\n",
    "\n",
    "## 4. Test Model \n",
    "- Evaluate Model with Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import pprint\n",
    "from termcolor import colored, cprint\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "from itertools import permutations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import tensorflow as tf\n",
    "\n",
    "# to print pandas dataframe\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "### Load Data Files using pandas\n",
    "- train : Knowledge Graph file with triple form\n",
    "- test : query with triple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'ex_neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/example_6.txt', sep='\\t', names=['subj','pred','obj'])\n",
    "test = pd.read_csv('./data/example_test_6.txt', sep='\\t', names=['subj','pred','obj'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>pred</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BART</td>\n",
       "      <td>nationality</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>hasCitizen</td>\n",
       "      <td>BART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>locatedIn</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BART</td>\n",
       "      <td>hasFather</td>\n",
       "      <td>HOMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOMMER</td>\n",
       "      <td>nationality</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subj          pred      obj\n",
       "0     BART   nationality      USA\n",
       "1      USA    hasCitizen     BART\n",
       "2     BART  placeOfBirth  NEWYORK\n",
       "3  NEWYORK     locatedIn      USA\n",
       "4     BART     hasFather   HOMMER\n",
       "5   HOMMER   nationality      USA"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule template and parsing using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(string):\n",
    "    \"\"\"\n",
    "    - function: trim whitespaces\n",
    "    :param string: an input string\n",
    "    \n",
    "    :return: the string without trailing whitespaces\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\A\\s+|\\s+\\Z\", \"\", string)\n",
    "\n",
    "def load_from_file(path, rule_template=False):\n",
    "    \"\"\"\n",
    "    - function: load and parsing file\n",
    "    :param path: file's location \n",
    "    :param rule_template: check rule file\n",
    "    \n",
    "    :return : parsed kb or rule template\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [x for x in text if not x.startswith(\"%\") and x.strip() != \"\"]\n",
    "        text = \"\".join(text)\n",
    "        rules = [x for x in re.split(\"\\.\\n|\\.\\Z\", text) if x != \"\" and\n",
    "                 x != \"\\n\" and not x.startswith(\"%\")]\n",
    "        print('rules')\n",
    "        print(rules)\n",
    "        kb = parse_rules(rules, rule_template=rule_template)\n",
    "        return kb\n",
    "    \n",
    "def parse_rules(rules, delimiter=\"#####\", rule_template=False):\n",
    "    \"\"\"\n",
    "    - function: read file and parse rules\n",
    "    :param rules: rules which need parsing\n",
    "    :param delimiter: a line delimiter\n",
    "    \n",
    "    :return: parsed rules\n",
    "    \"\"\"\n",
    "    kb = []\n",
    "    for rule in rules:\n",
    "        print(rule)\n",
    "        if rule_template:\n",
    "            splits = re.split(\"\\A\\n?([0-9]?[0-9]+)\", rule)\n",
    "            print(splits)\n",
    "            # fixme: should be 0 and 1 respectively\n",
    "            num = int(splits[1])\n",
    "            rule = splits[2]\n",
    "        rule = re.sub(\":-\", delimiter, rule)\n",
    "        print('rule1')\n",
    "        print(rule)\n",
    "        rule = re.sub(\"\\),\", \")\"+delimiter, rule)\n",
    "        print('rule2')\n",
    "        print(rule)\n",
    "        rule = [trim(x) for x in rule.split(delimiter)]\n",
    "        rule = [x for x in rule if x != \"\"]\n",
    "        if len(rule) > 0:\n",
    "            atoms = []\n",
    "            for atom in rule:\n",
    "                splits = atom.split(\"(\")\n",
    "                predicate = splits[0]\n",
    "                args = [x for x in re.split(\"\\s?,\\s?|\\)\", splits[1]) if x != \"\"]\n",
    "                atoms.append((predicate, args[0], args[1]))\n",
    "            #@jaeseung : get augment number\n",
    "            atoms.append(num)\n",
    "            \n",
    "            kb.append(atoms)\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rules\n",
      "['3\\t#1(X, Y) :- #2(Y, X)', '3\\t#1(X, Y) :- #2(X, Z),#3(Z, Y)']\n",
      "3\t#1(X, Y) :- #2(Y, X)\n",
      "['', '3', '\\t#1(X, Y) :- #2(Y, X)']\n",
      "rule1\n",
      "\t#1(X, Y) ##### #2(Y, X)\n",
      "rule2\n",
      "\t#1(X, Y) ##### #2(Y, X)\n",
      "3\t#1(X, Y) :- #2(X, Z),#3(Z, Y)\n",
      "['', '3', '\\t#1(X, Y) :- #2(X, Z),#3(Z, Y)']\n",
      "rule1\n",
      "\t#1(X, Y) ##### #2(X, Z),#3(Z, Y)\n",
      "rule2\n",
      "\t#1(X, Y) ##### #2(X, Z)######3(Z, Y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('#1', 'X', 'Y'), ('#2', 'Y', 'X'), 3],\n",
       " [('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = load_from_file('./data/example.nlt', rule_template=True)\n",
    "# rules = load_from_file('./new_data/kinship.nlt', rule_template=True)\n",
    "# rules = load_from_file('./new_data/train2.nlt', rule_template=True)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary from Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sym_dict = {}\n",
    "sym2id_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BART', 'HOMMER', 'NEWYORK', 'USA']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get entities from train & test data \n",
    "entities_list = sorted(set(train.subj.values).union(set(train.obj.values)).union(set(test.subj.values)).union(set(test.obj.values)))\n",
    "entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hasCitizen', 'hasFather', 'locatedIn', 'nationality', 'placeOfBirth']\n",
      "['#1_0_0', '#1_0_1', '#1_0_2', '#2_0_0', '#2_0_1', '#2_0_2', '#1_1_0', '#1_1_1', '#1_1_2', '#2_1_0', '#2_1_1', '#2_1_2', '#3_1_0', '#3_1_1', '#3_1_2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['#1_0_0',\n",
       " '#1_0_1',\n",
       " '#1_0_2',\n",
       " '#1_1_0',\n",
       " '#1_1_1',\n",
       " '#1_1_2',\n",
       " '#2_0_0',\n",
       " '#2_0_1',\n",
       " '#2_0_2',\n",
       " '#2_1_0',\n",
       " '#2_1_1',\n",
       " '#2_1_2',\n",
       " '#3_1_0',\n",
       " '#3_1_1',\n",
       " '#3_1_2',\n",
       " 'hasCitizen',\n",
       " 'hasFather',\n",
       " 'locatedIn',\n",
       " 'nationality',\n",
       " 'placeOfBirth']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate_list = sorted(set(train.pred.values).union(set(test.pred.values)))\n",
    "print(predicate_list)\n",
    "\n",
    "tmp_pred_list = []\n",
    "# get rule's predicates\n",
    "for i, rule in enumerate(rules):\n",
    "    # get all body\n",
    "    for r in rule[:-1]:\n",
    "        # if rule has augment\n",
    "        for j in range(rule[-1]):\n",
    "            suffix = '_' + str(i) + '_' + str(j)\n",
    "            tmp_pred_list.append(r[0]+suffix)\n",
    "            \n",
    "print(tmp_pred_list)\n",
    "\n",
    "predicate_list = sorted(set(predicate_list).union(set(tmp_pred_list)))\n",
    "predicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "id2sym_dict[0] = 'UNK'\n",
    "sym2id_dict['UNK'] = 0\n",
    "\n",
    "# constant_ids = []\n",
    "predicate_ids = []\n",
    "\n",
    "for i, p in enumerate(predicate_list):\n",
    "    id2sym_dict[i+1] = p\n",
    "    sym2id_dict[p] = i+1\n",
    "    predicate_ids.append(i+1)\n",
    "    \n",
    "# @blocked : remove entities from dict\n",
    "# for i, e in enumerate(entities_list):\n",
    "#     id2sym_dict[i+len(predicate_list)+1] = e\n",
    "#     sym2id_dict[e] = i+len(predicate_list)+1\n",
    "#     constant_ids.append(i+len(predicate_list)+1)\n",
    "    \n",
    "# print(constant_ids)\n",
    "# print()\n",
    "print(predicate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: '#1_0_0',\n",
       " 2: '#1_0_1',\n",
       " 3: '#1_0_2',\n",
       " 4: '#1_1_0',\n",
       " 5: '#1_1_1',\n",
       " 6: '#1_1_2',\n",
       " 7: '#2_0_0',\n",
       " 8: '#2_0_1',\n",
       " 9: '#2_0_2',\n",
       " 10: '#2_1_0',\n",
       " 11: '#2_1_1',\n",
       " 12: '#2_1_2',\n",
       " 13: '#3_1_0',\n",
       " 14: '#3_1_1',\n",
       " 15: '#3_1_2',\n",
       " 16: 'hasCitizen',\n",
       " 17: 'hasFather',\n",
       " 18: 'locatedIn',\n",
       " 19: 'nationality',\n",
       " 20: 'placeOfBirth'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2sym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " '#1_0_0': 1,\n",
       " '#1_0_1': 2,\n",
       " '#1_0_2': 3,\n",
       " '#1_1_0': 4,\n",
       " '#1_1_1': 5,\n",
       " '#1_1_2': 6,\n",
       " '#2_0_0': 7,\n",
       " '#2_0_1': 8,\n",
       " '#2_0_2': 9,\n",
       " '#2_1_0': 10,\n",
       " '#2_1_1': 11,\n",
       " '#2_1_2': 12,\n",
       " '#3_1_0': 13,\n",
       " '#3_1_1': 14,\n",
       " '#3_1_2': 15,\n",
       " 'hasCitizen': 16,\n",
       " 'hasFather': 17,\n",
       " 'locatedIn': 18,\n",
       " 'nationality': 19,\n",
       " 'placeOfBirth': 20}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym2id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions \n",
    "\n",
    "### pseudo code\n",
    "- goal: query (e.g. kim nationality korea)\n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "\n",
    "- 1. 주어진 rule template의 conclusion과 query를 매칭\n",
    "    - conclusion의 X,Y와 같은 Variable에 대하여   \n",
    "    query를 참조하여 X/kim, Y/korea와 같이 binding\n",
    "    - binding 후 각 Variable에 대하여 subject file과 object file 생성\n",
    "    - substitution dictionary에 key를 Variable로, value를 (subject file, object file)로 저장  \n",
    "\n",
    "\n",
    "\n",
    "- 2. 앞서 binding된 Variable을 참조하여 rule body의 Variable을 매칭\n",
    "    - conclusion의 #1(X,Y)를 통해 binding된 X에 대한 substitution을 참조하여  \n",
    "    #2(X,Z)와 같은 body의 variable인 Z를 binding하는 작업을 수행\n",
    "        - 위 경우에는 substitution으로부터 X의 subject file을 참조하여 Z에 대하여 binding 수행\n",
    "    - rule의 모든 body의 variable에 대하여 binding을 수행하고 결과를 substitution dictionary에 저장\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_triples(entity, kb):\n",
    "    '''\n",
    "    - function: generate subject & object file with given entities\n",
    "    :param entity: an entity to find subject & object file\n",
    "    :param kb: a knowledge graph\n",
    "    \n",
    "    :return: tuple of subject & object file\n",
    "    '''\n",
    "    # for debug\n",
    "#     print(colored('search_triples entity : ', 'magenta'), entity)\n",
    "    if isinstance(entity, list):\n",
    "        \n",
    "        print(\"isinstance is list\")\n",
    "        print(entity)\n",
    "        # 가능한 entity가 여러 개가 올 경우를 list로 처리\n",
    "        tmp = []\n",
    "        for ent in entity:\n",
    "            tmp.append(kb[kb['subj'] == ent])\n",
    "        \n",
    "        # 0815 update : when substitution is empty\n",
    "        if len(tmp) == 0:\n",
    "            subj_df = pd.DataFrame([], columns=['subj', 'pred', 'obj'])\n",
    "#             display(subj_df)\n",
    "        else:\n",
    "            subj_df = pd.concat(tmp)\n",
    "        tmp = []\n",
    "        for ent in entity:\n",
    "            tmp.append(kb[kb['obj'] == ent])\n",
    "\n",
    "        # 0815 update : when substitution is empty\n",
    "        if len(tmp) == 0:\n",
    "            obj_df = pd.DataFrame([], columns=['subj', 'pred', 'obj'])\n",
    "#             display(obj_df)\n",
    "        else:\n",
    "            obj_df = pd.concat(tmp)\n",
    "        return subj_df, obj_df\n",
    "    else : \n",
    "        print(\"else\")\n",
    "        print(entity)\n",
    "        # 하나의 entity가 올 경우\n",
    "        subj_df = kb[kb['subj'] == entity]\n",
    "        obj_df = kb[kb['obj'] == entity]\n",
    "        return subj_df, obj_df\n",
    "\n",
    "def join_triples(substitution, rule, pos):\n",
    "    '''\n",
    "    - function: join triples with rule to get proof path \n",
    "    :param substitution: a dictionary \n",
    "        - key: Variable / value: subject & object file\n",
    "    :param rule: a rule template\n",
    "    :param pos: check positive / negative sampling\n",
    "    \n",
    "    :return: proof paths generated with rule template\n",
    "    '''\n",
    "    proof_path = []\n",
    "    \n",
    "    tmp_result_df = None\n",
    "    head_df = None\n",
    "    body_df = None\n",
    "    \n",
    "    query_df = substitution['Q']\n",
    "    \n",
    "    for i in range(len(rule)-2): #rule 길이에 따른 join 횟수\n",
    "        # get variable from rule template\n",
    "        head_subj = rule[i][1] \n",
    "        head_obj = rule[i][2]\n",
    "        \n",
    "        body_subj = rule[i+1][1]\n",
    "        body_obj = rule[i+1][2]\n",
    "        \n",
    "        # check common variable\n",
    "        if head_subj == body_subj or head_subj == body_obj: # #1(X,Y) #2(Y,X)\n",
    "            comVar = head_subj\n",
    "        elif head_obj == body_subj or head_obj == body_obj:\n",
    "            comVar = head_obj\n",
    "\n",
    "        # get head dataframe using rule's variable\n",
    "        # if this is first step\n",
    "        if tmp_result_df is None:\n",
    "            # 0812 update : use query df only\n",
    "            head_df = query_df\n",
    "\n",
    "            # new DF foramt with [Common Variable, Triples]\n",
    "            tmp_head = []\n",
    "            for col in head_df.itertuples(index=False):\n",
    "                # check common variable\n",
    "                if comVar == head_subj:\n",
    "                    tmp_head.append([col.subj, [list(col)]])\n",
    "                elif comVar == head_obj:\n",
    "                    tmp_head.append([col.obj, [list(col)]])\n",
    "                \n",
    "            head_df = pd.DataFrame(tmp_head, columns=['comVar', 'triples'])\n",
    "\n",
    "        # if previous step's result df exists\n",
    "        else:\n",
    "            # if tmp_result_df exists update comVar\n",
    "            tmp_head = []\n",
    "            for col in tmp_result_df.itertuples(index=False):\n",
    "                # check common variable\n",
    "                if comVar == head_subj:\n",
    "                    tmp_head.append([col.triples[-1][0], col.triples])\n",
    "                elif comVar == head_obj:\n",
    "                    tmp_head.append([col.triples[-1][2], col.triples])\n",
    "            \n",
    "            head_df = pd.DataFrame(tmp_head, columns=['comVar', 'triples'])\n",
    "\n",
    "        # get body dataframe using rule's variable\n",
    "        body_df_subj = substitution[body_subj][1][0]\n",
    "        body_df_obj = substitution[body_obj][1][1]\n",
    "        \n",
    "        body_df = pd.merge(body_df_subj, body_df_obj, how='inner')\n",
    "        print('merge body df = ')\n",
    "        print(body_df)\n",
    "        \n",
    "        # new DF foramt with [Common Variable, Triples]\n",
    "        tmp_body = []\n",
    "        for col in body_df.itertuples(index=False):\n",
    "            # check common variable\n",
    "            if comVar == body_subj:\n",
    "                tmp_body.append([col.subj, [list(col)]])\n",
    "            elif comVar == body_obj:\n",
    "                tmp_body.append([col.obj, [list(col)]])\n",
    "\n",
    "        body_df = pd.DataFrame(tmp_body, columns=['comVar', 'triples'])\n",
    "        print('final body df = ')\n",
    "        print(body_df)\n",
    "\n",
    "        # merge two dataframe on common variable\n",
    "        print('before join')\n",
    "        print(head_df)\n",
    "        print(body_df)\n",
    "        tmp_result_df = pd.merge(head_df, body_df, how='inner', on='comVar', suffixes=('_left', '_right'))\n",
    "        print('after join tmp_result_df = ')\n",
    "        print(tmp_result_df)\n",
    "\n",
    "        # for debug\n",
    "#         print(colored('tmp result', 'red', attrs=['bold']))\n",
    "#         display(tmp_result_df)\n",
    "        \n",
    "        # Todo : need fix\n",
    "        # check left and right triples are same\n",
    "        check_same_list = []\n",
    "        for row, col in tmp_result_df.iterrows():\n",
    "            if sorted(col['triples_left']) != sorted(col['triples_right']):\n",
    "                check_same_list.append(row)\n",
    "            \n",
    "        tmp_result_df = tmp_result_df.iloc[check_same_list]\n",
    "\n",
    "        # merge left & right triples columns into one column\n",
    "        tmp_result_df['triples'] = tmp_result_df['triples_left'] + tmp_result_df['triples_right']\n",
    "        del tmp_result_df['triples_left']\n",
    "        del tmp_result_df['triples_right']\n",
    "        \n",
    "        # for debug\n",
    "#         print(colored('result', 'red', attrs=['bold']))\n",
    "#         display(tmp_result_df)\n",
    "#         print()\n",
    "        \n",
    "    # if rule ends, return final result df\n",
    "    result = tmp_result_df\n",
    "    print('del triple_left, right, + +')\n",
    "    print(result)\n",
    "    # check result dataframe and generate sim_id proof paths\n",
    "    for col in result.itertuples(index=False):\n",
    "        tmp_path = []\n",
    "        for i in range(len(rule)-1):\n",
    "            tmp_path.append((rule[i][0], col.triples[i][1]))\n",
    "        \n",
    "        if tmp_path not in proof_path:\n",
    "            proof_path.append(tmp_path)\n",
    "    print('proof_path = ')\n",
    "    print(proof_path)\n",
    "    return proof_path\n",
    "\n",
    "def unify(goal, rule, kb, depth=0, substitution={}, neg_per_pos=4):\n",
    "    '''\n",
    "    - function: \n",
    "        1. Unify Variables and store information in substitution dictionary\n",
    "        2. Check Common Variable from Rules and Join each Triples\n",
    "        \n",
    "    :param goal: a query triple (e.g. [kim nationality korea])\n",
    "    :parma rule: a given rule template (e.g. [#1(X,Y) :- #2(Y,X), 2])\n",
    "    :param depth: an integer indicates rule depth\n",
    "    :param substitution: a dictionary which has information of unified variables\n",
    "        - key: Variable / value: subject & object file\n",
    "    :return: proof paths generated by Symbolic Unification \n",
    "    '''\n",
    "            \n",
    "    # for debug\n",
    "#     print(colored('goal : ', 'red'))\n",
    "#     display(goal)\n",
    "#     print(colored('rule : ', 'green'), rule)\n",
    "#     print(colored('substitution : ', 'cyan'))\n",
    "#     for k, v in substitution.items():\n",
    "#         print('key : ', k)\n",
    "#         display(v)\n",
    "#     print()\n",
    "    \n",
    "    # if substitution is empty\n",
    "    if len(substitution) == 0 :\n",
    "        print('depth : ', depth)\n",
    "#         substitution['Q'] = goal.to_frame().transpose()\n",
    "        substitution['Q'] = pd.DataFrame(goal, index=['subj','pred','obj']).transpose()\n",
    "        # subject variable binding\n",
    "        if rule[depth][1] not in substitution.keys():\n",
    "#             substitution[rule[depth][1]] = [goal['subj'], search_triples(goal['subj'], kb)]\n",
    "            substitution[rule[depth][1]] = [goal[0], search_triples(goal[0], kb)]\n",
    "            print(goal[0])\n",
    "        # object variable binding\n",
    "        if rule[depth][2] not in substitution.keys():\n",
    "#             substitution[rule[depth][2]] = [goal['obj'], search_triples(goal['obj'], kb)]\n",
    "            substitution[rule[depth][2]] = [goal[2], search_triples(goal[2], kb)]\n",
    "            print(goal[2])\n",
    "    else:\n",
    "        # for debug\n",
    "#         print('*'*20)\n",
    "#         print(goal)\n",
    "        print('depth : ', depth)\n",
    "        # check which variable needs binding\n",
    "        # check subject variable\n",
    "        if rule[depth][1] not in substitution.keys():\n",
    "            tmp_var = list(set(goal.subj.values))\n",
    "            substitution[rule[depth][1]] = [tmp_var, search_triples(tmp_var, kb)]\n",
    "        # object subject variable\n",
    "        if rule[depth][2] not in substitution.keys():\n",
    "            tmp_var = list(set(goal.obj.values))\n",
    "            substitution[rule[depth][2]] = [tmp_var, search_triples(tmp_var, kb)]\n",
    "    \n",
    "    # if last rule body return sim_ids\n",
    "    if depth == len(rule)-2:\n",
    "        # 0812 update : negative sampling\n",
    "        \n",
    "        # Todo : if there are only few predicate exists - like countries[locatedIn, neighbourOf] \n",
    "        proof_paths = []\n",
    "        # first : positive paths\n",
    "        proof_paths.append(join_triples(substitution, rule, pos=True))\n",
    "        # iteration for negative random sampling\n",
    "        \n",
    "        return proof_paths\n",
    "\n",
    "    else:\n",
    "        depth = depth + 1\n",
    "        \n",
    "        # check common variable\n",
    "        # if first variable is common\n",
    "        if rule[depth-1][1] == rule[depth][1]:\n",
    "            print('##[1],[1]##')\n",
    "            print('substitution = ')\n",
    "            pprint(substitution)\n",
    "#             print('sub-goal')\n",
    "#             print('substitution[rule[depth-1][1]]')\n",
    "#             pprint(substitution[rule[depth-1][1]])\n",
    "#             print('substitution[rule[depth-1][1]][1]')\n",
    "#             pprint(substitution[rule[depth-1][1]][1])\n",
    "#             print('substitution[rule[depth-1][1]][1][0]')\n",
    "#             pprint(substitution[rule[depth-1][1]][1][0])\n",
    "            print('sub-goal = ')\n",
    "            pprint(substitution[rule[depth-1][1]][1][0])\n",
    "            return unify(substitution[rule[depth-1][1]][1][0], rule, kb, depth, substitution, neg_per_pos)\n",
    "        if rule[depth-1][1] == rule[depth][2]:\n",
    "            print('##[1],[2]##')\n",
    "            print('substitution = ')\n",
    "            pprint(substitution)\n",
    "#             print('substitution')\n",
    "#             pprint(substitution)\n",
    "#             print('sub-goal')\n",
    "#             print('substitution[rule[depth-1][1]]')\n",
    "#             pprint(substitution[rule[depth-1][1]])\n",
    "#             print('substitution[rule[depth-1][1]][1]')\n",
    "#             pprint(substitution[rule[depth-1][1]][1])\n",
    "#             print('substitution[rule[depth-1][1]][1][1]')\n",
    "#             pprint(substitution[rule[depth-1][1]][1][1])\n",
    "            print('sub-goal = ')\n",
    "            pprint(substitution[rule[depth-1][1]][1][1])\n",
    "            return unify(substitution[rule[depth-1][1]][1][1], rule, kb, depth, substitution, neg_per_pos)\n",
    "        \n",
    "        # if second variable is common\n",
    "        if rule[depth-1][2] == rule[depth][1]:\n",
    "            print('##[2],[1]##')\n",
    "            print('substitution = ')\n",
    "            pprint(substitution)\n",
    "            print('sub-goal = ')\n",
    "            pprint(substitution[rule[depth-1][2]][1][0])\n",
    "            return unify(substitution[rule[depth-1][2]][1][0], rule, kb, depth, substitution, neg_per_pos)\n",
    "        if rule[depth-1][2] == rule[depth][2]:\n",
    "            print('##[2],[2]##')\n",
    "            print('substitution = ')\n",
    "            pprint(substitution)\n",
    "            print('sub-goal = ')\n",
    "            pprint(substitution[rule[depth-1][2]][1][1])\n",
    "            return unify(substitution[rule[depth-1][2]][1][1], rule, kb, depth, substitution, neg_per_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Symbolic Unification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change entities to generate negative triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pos_per_batch = 1\n",
    "sampling_scheme = False\n",
    "\n",
    "if sampling_scheme:\n",
    "    neg_per_pos = 4\n",
    "else: \n",
    "    neg_per_pos = 2\n",
    "\n",
    "batch_size = pos_per_batch + (pos_per_batch * neg_per_pos)\n",
    "print(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#1_0_0',\n",
       " '#1_0_1',\n",
       " '#1_0_2',\n",
       " '#1_1_0',\n",
       " '#1_1_1',\n",
       " '#1_1_2',\n",
       " '#2_0_0',\n",
       " '#2_0_1',\n",
       " '#2_0_2',\n",
       " '#2_1_0',\n",
       " '#2_1_1',\n",
       " '#2_1_2',\n",
       " '#3_1_0',\n",
       " '#3_1_1',\n",
       " '#3_1_2',\n",
       " 'hasCitizen',\n",
       " 'hasFather',\n",
       " 'locatedIn',\n",
       " 'nationality',\n",
       " 'placeOfBirth']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicate_set = set([pred for pred in predicate_list if pred[0] != '#'])\n",
    "# predicate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth :  0\n",
      "else\n",
      "BART\n",
      "BART\n",
      "else\n",
      "USA\n",
      "USA\n",
      "##[1],[2]##\n",
      "substitution = \n",
      "{'Q':    subj         pred  obj\n",
      "0  BART  nationality  USA,\n",
      " 'X': ['BART',\n",
      "       (   subj          pred      obj\n",
      "0  BART   nationality      USA\n",
      "2  BART  placeOfBirth  NEWYORK\n",
      "4  BART     hasFather   HOMMER,\n",
      "          subj        pred   obj\n",
      "1  USA  hasCitizen  BART)],\n",
      " 'Y': ['USA',\n",
      "       (  subj        pred   obj\n",
      "1  USA  hasCitizen  BART,\n",
      "              subj         pred  obj\n",
      "0     BART  nationality  USA\n",
      "3  NEWYORK    locatedIn  USA\n",
      "5   HOMMER  nationality  USA)]}\n",
      "sub-goal = \n",
      "  subj        pred   obj\n",
      "1  USA  hasCitizen  BART\n",
      "depth :  1\n",
      "merge body df = \n",
      "  subj        pred   obj\n",
      "0  USA  hasCitizen  BART\n",
      "final body df = \n",
      "  comVar                    triples\n",
      "0   BART  [[USA, hasCitizen, BART]]\n",
      "before join\n",
      "  comVar                     triples\n",
      "0   BART  [[BART, nationality, USA]]\n",
      "  comVar                    triples\n",
      "0   BART  [[USA, hasCitizen, BART]]\n",
      "after join tmp_result_df = \n",
      "  comVar                triples_left              triples_right\n",
      "0   BART  [[BART, nationality, USA]]  [[USA, hasCitizen, BART]]\n",
      "del triple_left, right, + +\n",
      "  comVar                                            triples\n",
      "0   BART  [[BART, nationality, USA], [USA, hasCitizen, B...\n",
      "proof_path = \n",
      "[[('#1', 'nationality'), ('#2', 'hasCitizen')]]\n",
      "depth :  0\n",
      "else\n",
      "BART\n",
      "BART\n",
      "else\n",
      "USA\n",
      "USA\n",
      "##[1],[1]##\n",
      "substitution = \n",
      "{'Q':    subj         pred  obj\n",
      "0  BART  nationality  USA,\n",
      " 'X': ['BART',\n",
      "       (   subj          pred      obj\n",
      "0  BART   nationality      USA\n",
      "2  BART  placeOfBirth  NEWYORK\n",
      "4  BART     hasFather   HOMMER,\n",
      "          subj        pred   obj\n",
      "1  USA  hasCitizen  BART)],\n",
      " 'Y': ['USA',\n",
      "       (  subj        pred   obj\n",
      "1  USA  hasCitizen  BART,\n",
      "              subj         pred  obj\n",
      "0     BART  nationality  USA\n",
      "3  NEWYORK    locatedIn  USA\n",
      "5   HOMMER  nationality  USA)]}\n",
      "sub-goal = \n",
      "   subj          pred      obj\n",
      "0  BART   nationality      USA\n",
      "2  BART  placeOfBirth  NEWYORK\n",
      "4  BART     hasFather   HOMMER\n",
      "depth :  1\n",
      "isinstance is list\n",
      "['USA', 'NEWYORK', 'HOMMER']\n",
      "##[2],[1]##\n",
      "substitution = \n",
      "{'Q':    subj         pred  obj\n",
      "0  BART  nationality  USA,\n",
      " 'X': ['BART',\n",
      "       (   subj          pred      obj\n",
      "0  BART   nationality      USA\n",
      "2  BART  placeOfBirth  NEWYORK\n",
      "4  BART     hasFather   HOMMER,\n",
      "          subj        pred   obj\n",
      "1  USA  hasCitizen  BART)],\n",
      " 'Y': ['USA',\n",
      "       (  subj        pred   obj\n",
      "1  USA  hasCitizen  BART,\n",
      "              subj         pred  obj\n",
      "0     BART  nationality  USA\n",
      "3  NEWYORK    locatedIn  USA\n",
      "5   HOMMER  nationality  USA)],\n",
      " 'Z': [['USA', 'NEWYORK', 'HOMMER'],\n",
      "       (      subj         pred   obj\n",
      "1      USA   hasCitizen  BART\n",
      "3  NEWYORK    locatedIn   USA\n",
      "5   HOMMER  nationality   USA,\n",
      "              subj          pred      obj\n",
      "0     BART   nationality      USA\n",
      "3  NEWYORK     locatedIn      USA\n",
      "5   HOMMER   nationality      USA\n",
      "2     BART  placeOfBirth  NEWYORK\n",
      "4     BART     hasFather   HOMMER)]}\n",
      "sub-goal = \n",
      "      subj         pred   obj\n",
      "1      USA   hasCitizen  BART\n",
      "3  NEWYORK    locatedIn   USA\n",
      "5   HOMMER  nationality   USA\n",
      "depth :  2\n",
      "merge body df = \n",
      "   subj          pred      obj\n",
      "0  BART   nationality      USA\n",
      "1  BART  placeOfBirth  NEWYORK\n",
      "2  BART     hasFather   HOMMER\n",
      "final body df = \n",
      "  comVar                          triples\n",
      "0   BART       [[BART, nationality, USA]]\n",
      "1   BART  [[BART, placeOfBirth, NEWYORK]]\n",
      "2   BART      [[BART, hasFather, HOMMER]]\n",
      "before join\n",
      "  comVar                     triples\n",
      "0   BART  [[BART, nationality, USA]]\n",
      "  comVar                          triples\n",
      "0   BART       [[BART, nationality, USA]]\n",
      "1   BART  [[BART, placeOfBirth, NEWYORK]]\n",
      "2   BART      [[BART, hasFather, HOMMER]]\n",
      "after join tmp_result_df = \n",
      "  comVar                triples_left                    triples_right\n",
      "0   BART  [[BART, nationality, USA]]       [[BART, nationality, USA]]\n",
      "1   BART  [[BART, nationality, USA]]  [[BART, placeOfBirth, NEWYORK]]\n",
      "2   BART  [[BART, nationality, USA]]      [[BART, hasFather, HOMMER]]\n",
      "merge body df = \n",
      "      subj         pred  obj\n",
      "0  NEWYORK    locatedIn  USA\n",
      "1   HOMMER  nationality  USA\n",
      "final body df = \n",
      "    comVar                       triples\n",
      "0  NEWYORK   [[NEWYORK, locatedIn, USA]]\n",
      "1   HOMMER  [[HOMMER, nationality, USA]]\n",
      "before join\n",
      "    comVar                                            triples\n",
      "0  NEWYORK  [[BART, nationality, USA], [BART, placeOfBirth...\n",
      "1   HOMMER  [[BART, nationality, USA], [BART, hasFather, H...\n",
      "    comVar                       triples\n",
      "0  NEWYORK   [[NEWYORK, locatedIn, USA]]\n",
      "1   HOMMER  [[HOMMER, nationality, USA]]\n",
      "after join tmp_result_df = \n",
      "    comVar                                       triples_left  \\\n",
      "0  NEWYORK  [[BART, nationality, USA], [BART, placeOfBirth...   \n",
      "1   HOMMER  [[BART, nationality, USA], [BART, hasFather, H...   \n",
      "\n",
      "                  triples_right  \n",
      "0   [[NEWYORK, locatedIn, USA]]  \n",
      "1  [[HOMMER, nationality, USA]]  \n",
      "del triple_left, right, + +\n",
      "    comVar                                            triples\n",
      "0  NEWYORK  [[BART, nationality, USA], [BART, placeOfBirth...\n",
      "1   HOMMER  [[BART, nationality, USA], [BART, hasFather, H...\n",
      "proof_path = \n",
      "[[('#1', 'nationality'), ('#2', 'placeOfBirth'), ('#3', 'locatedIn')], [('#1', 'nationality'), ('#2', 'hasFather'), ('#3', 'nationality')]]\n",
      "symbolic time :  0:00:00.202935\n",
      "converting time :  0:00:00\n"
     ]
    }
   ],
   "source": [
    "# 0818 : generate batch Function\n",
    "def generate_batch(query, rules, neg_per_pos):\n",
    "    '''\n",
    "    - function: generate batch from given query triples & rule templates\n",
    "        \n",
    "    :param query: query triples with dataframe format\n",
    "    :parma rule: a given rule template (e.g. [#1(X,Y) :- #2(Y,X), 2])\n",
    "    :param neg_per_pos: a number of negative data for each postive data\n",
    "    :return: proof paths with (r1, r2) format\n",
    "    '''    \n",
    "\n",
    "    total_syms_list_r1 = []\n",
    "    total_syms_list_r2 = []\n",
    "\n",
    "    # debug : to check time\n",
    "    start = datetime.now()\n",
    "\n",
    "    for col in query.itertuples(index=False):\n",
    "\n",
    "        # if use test\n",
    "#         sim_syms_list_r1 = [[[] for rule in rules] for i in range(neg_per_pos+1)]\n",
    "#         sim_syms_list_r2 = [[[] for rule in rules] for i in range(neg_per_pos+1)]\n",
    "\n",
    "        # if use new_test\n",
    "        sim_syms_list_r1 = [[[] for rule in rules]] ###[[[], []]] rule predicate\n",
    "        sim_syms_list_r2 = [[[] for rule in rules]] ###[[[], []]] kb predicate\n",
    "        \n",
    "        for i, rule in enumerate(rules):           \n",
    "            sim_syms_list = unify(list(col), rule, train, substitution={}, neg_per_pos=neg_per_pos)\n",
    "            for k, sim_syms in enumerate(sim_syms_list):\n",
    "                for sim in sim_syms:\n",
    "                    tmp_syms_list_r1 = []\n",
    "                    tmp_syms_list_r2 = []\n",
    "                    for j in range(rule[-1]):\n",
    "                        suffix = '_' + str(i) + '_' + str(j)\n",
    "                        tmp_ids_r1 = []\n",
    "                        tmp_ids_r2 = []\n",
    "\n",
    "                        for r1, r2 in sim:\n",
    "                            tmp_ids_r1.append(r1+suffix)\n",
    "                            tmp_ids_r2.append(r2)\n",
    "\n",
    "                        tmp_syms_list_r1.append(tmp_ids_r1)\n",
    "                        tmp_syms_list_r2.append(tmp_ids_r2)\n",
    "\n",
    "                    sim_syms_list_r1[k][i].append(tmp_syms_list_r1)\n",
    "                    sim_syms_list_r2[k][i].append(tmp_syms_list_r2)\n",
    "\n",
    "        for k in range(len(sim_syms_list)):\n",
    "            total_syms_list_r1.append((sim_syms_list_r1[k]))\n",
    "            total_syms_list_r2.append((sim_syms_list_r2[k]))\n",
    "\n",
    "    end = datetime.now()\n",
    "    print('symbolic time : ', end-start)\n",
    "    \n",
    "    # convert symbolic into ids\n",
    "    total_ids_list_r1 = []\n",
    "    total_ids_list_r2 = []\n",
    "\n",
    "    # debug : to check time\n",
    "    start = datetime.now()\n",
    "\n",
    "    for sim_syms_list in total_syms_list_r1:\n",
    "        tmp_sim_ids_lists = []\n",
    "        for sim_syms_l in sim_syms_list:\n",
    "            sim_ids_list = []\n",
    "            for sim_syms in sim_syms_l:\n",
    "                sim_ids = []\n",
    "                for sim in sim_syms:\n",
    "                    sim_ids.append([sym2id_dict[i] for i in sim])\n",
    "                sim_ids_list.append(sim_ids)\n",
    "            tmp_sim_ids_lists.append(sim_ids_list)    \n",
    "        total_ids_list_r1.append(tmp_sim_ids_lists)\n",
    "\n",
    "    for sim_syms_list in total_syms_list_r2:\n",
    "        tmp_sim_ids_lists = []\n",
    "        for sim_syms_l in sim_syms_list:\n",
    "            sim_ids_list = []\n",
    "            for sim_syms in sim_syms_l:\n",
    "                sim_ids = []\n",
    "                for sim in sim_syms:\n",
    "                    sim_ids.append([sym2id_dict[i] for i in sim])\n",
    "                sim_ids_list.append(sim_ids)\n",
    "            tmp_sim_ids_lists.append(sim_ids_list)    \n",
    "        total_ids_list_r2.append(tmp_sim_ids_lists)\n",
    "\n",
    "    end = datetime.now() \n",
    "    print('converting time : ', end-start)\n",
    "    \n",
    "    return total_ids_list_r1, total_ids_list_r2, total_syms_list_r1, total_syms_list_r2\n",
    "#    return total_syms_list_r1, total_syms_list_r2\n",
    "    \n",
    "    \n",
    "total_ids_list_r1, total_ids_list_r2, total_syms_list_r1, total_syms_list_r2 = generate_batch(test, rules, neg_per_pos)\n",
    "#total_ids_list_r1, total_ids_list_r2 = generate_batch(new_test, rules, predicate_set, neg_per_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic Unification Result\n",
    "- two lists \n",
    "    - r1 : rule template predicates\n",
    "    - r2 : unified predicates\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(colored('sim_syms', 'red', attrs=['bold']))\n",
    "# for sim_syms_list in total_syms_list_r1:\n",
    "#     for sim_syms_l in sim_syms_list:\n",
    "#         for sim_syms in sim_syms_l:\n",
    "#             for sim in sim_syms:\n",
    "#                 print(sim)\n",
    "#         print()\n",
    "\n",
    "# for sim_syms_list in total_syms_list_r2:\n",
    "#     for sim_syms_l in sim_syms_list:\n",
    "#         for sim_syms in sim_syms_l:\n",
    "#             for sim in sim_syms:\n",
    "#                 print(sim)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Symbolic Unification Result from string into index\n",
    "- convert string into index using dictionary\n",
    "- two lists\n",
    "    - r1 : rule template predicates\n",
    "    - r2 : unified predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total_ids_list_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total_ids_list_r2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def data_filter(data):\n",
    "    boolian_variable = False\n",
    "    for template in data:\n",
    "        if len(template) != 0:\n",
    "            boolian_variable = boolian_variable or True\n",
    "        else:\n",
    "            boolian_variable = boolian_variable or False\n",
    "    return boolian_variable\n",
    "print(len(total_ids_list_r2))\n",
    "print(len(total_ids_list_r1))\n",
    "total_ids_list_r1 = list(filter(data_filter, total_ids_list_r1))\n",
    "total_ids_list_r2 = list(filter(data_filter, total_ids_list_r2))\n",
    "print(len(total_ids_list_r1))\n",
    "print(len(total_ids_list_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make neg data & append padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shin\n",
    "atoms_each_template = []\n",
    "for rule in rules:\n",
    "    atoms_each_template.append(len(rule)-1)\n",
    "    \n",
    "atoms_each_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = rules[0][-1]\n",
    "num_paths = []\n",
    "num_templates = len(total_ids_list_r1[0])\n",
    "\n",
    "time1 = datetime.now()\n",
    "\n",
    "for idx, i in enumerate(total_ids_list_r1):\n",
    "    for j in range(num_templates):\n",
    "        num_paths.append(np.array(i[j]).shape[0])\n",
    "max_path = max(num_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkdict_sym = {}\n",
    "\n",
    "for idx, (query1,query2) in enumerate(zip(total_syms_list_r1, total_syms_list_r2)):\n",
    "    for (template1,template2) in zip(query1,query2):\n",
    "        for (path1,path2) in zip(template1,template2):\n",
    "            for (aug1,aug2) in zip(path1,path2):\n",
    "                for (atom1,atom2) in zip(aug1,aug2):\n",
    "                    atom1_ = atom1.split('_')\n",
    "                    key = atom1_[0]+'_'+atom1_[1]\n",
    "                    if key not in checkdict_sym:\n",
    "                        checkdict_sym[key] = []\n",
    "                    if atom2 not in checkdict_sym[key]:\n",
    "                        checkdict_sym[key].append(atom2)\n",
    "keys = list(checkdict_sym.keys())                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#1_0', '#2_0', '#1_1', '#2_1', '#3_1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#1_0': ['nationality'],\n",
       " '#2_0': ['hasCitizen'],\n",
       " '#1_1': ['nationality'],\n",
       " '#2_1': ['placeOfBirth', 'hasFather'],\n",
       " '#3_1': ['locatedIn', 'nationality']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkdict_sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### head body 랜덤 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "new_total_ids_list_r1 = []\n",
    "new_total_ids_list_r2 = []\n",
    "neg_per_pos = 2\n",
    "for idx, (query1,query2) in enumerate(zip(total_ids_list_r1, total_ids_list_r2)):\n",
    "    if idx % 100 ==0:\n",
    "        print(idx)\n",
    "    #positive\n",
    "    new_total_ids_list_r1.append(query1)\n",
    "    new_total_ids_list_r2.append(query2)\n",
    "    for i in range(neg_per_pos):\n",
    "        new_query2 = copy.deepcopy(query2)\n",
    "        for template_idx, template1 in enumerate(new_query2):\n",
    "            if len(template1) == 0:\n",
    "                continue\n",
    "            for path_idx, path1 in enumerate(template1):\n",
    "                 for aug_idx, aug1 in enumerate(path1):\n",
    "                    if aug_idx == 0:\n",
    "                        for atom_idx, atom1 in enumerate(aug1):\n",
    "                            #find Corresponding rule predicate\n",
    "                            rule_pred = id2sym_dict[query1[template_idx][path_idx][aug_idx][atom_idx]]\n",
    "                            #Convert to key \n",
    "                            rule_pred = rule_pred.split('_')\n",
    "                            key = rule_pred[0]+'_'+rule_pred[1]\n",
    "\n",
    "                            #Excluding positive key\n",
    "                            filtered_keys = copy.deepcopy(keys)\n",
    "                            filtered_keys.remove(key)\n",
    "                            #choice new key rendomly\n",
    "                            new_key = random.choice(filtered_keys)\n",
    "                            #choice new id randomly\n",
    "                            new_id = sym2id_dict[random.choice(checkdict_sym[new_key])]\n",
    "                            new_query2[template_idx][path_idx][aug_idx][atom_idx] = new_id\n",
    "                    else :\n",
    "                        neg_path = np.full((augment,len(aug1)), new_query2[template_idx][path_idx][0]).tolist()\n",
    "                        new_query2[template_idx][path_idx] = neg_path\n",
    "                        break\n",
    "\n",
    "        new_total_ids_list_r1.append(query1)\n",
    "        new_total_ids_list_r2.append(new_query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: '#1_0_0',\n",
       " 2: '#1_0_1',\n",
       " 3: '#1_0_2',\n",
       " 4: '#1_1_0',\n",
       " 5: '#1_1_1',\n",
       " 6: '#1_1_2',\n",
       " 7: '#2_0_0',\n",
       " 8: '#2_0_1',\n",
       " 9: '#2_0_2',\n",
       " 10: '#2_1_0',\n",
       " 11: '#2_1_1',\n",
       " 12: '#2_1_2',\n",
       " 13: '#3_1_0',\n",
       " 14: '#3_1_1',\n",
       " 15: '#3_1_2',\n",
       " 16: 'hasCitizen',\n",
       " 17: 'hasFather',\n",
       " 18: 'locatedIn',\n",
       " 19: 'nationality',\n",
       " 20: 'placeOfBirth'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2sym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[[19, 16], [19, 16], [19, 16]]],\n",
       "  [[[19, 20, 18], [19, 20, 18], [19, 20, 18]],\n",
       "   [[19, 17, 19], [19, 17, 19], [19, 17, 19]]]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ids_list_r2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[[19, 16], [19, 16], [19, 16]]],\n",
       "  [[[19, 20, 18], [19, 20, 18], [19, 20, 18]],\n",
       "   [[19, 17, 19], [19, 17, 19], [19, 17, 19]]]],\n",
       " [[[[19, 19], [19, 19], [19, 19]]],\n",
       "  [[[16, 19, 19], [16, 19, 19], [16, 19, 19]],\n",
       "   [[19, 19, 16], [19, 19, 16], [19, 19, 16]]]],\n",
       " [[[[19, 19], [19, 19], [19, 19]]],\n",
       "  [[[18, 19, 16], [18, 19, 16], [18, 19, 16]],\n",
       "   [[16, 16, 19], [16, 16, 19], [16, 16, 19]]]]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_total_ids_list_r2[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "augment = rules[0][-1]\n",
    "num_paths = []\n",
    "num_templates = len(total_ids_list_r1[0])\n",
    "\n",
    "time1 = datetime.now()\n",
    "\n",
    "for idx, i in enumerate(new_total_ids_list_r1):\n",
    "    for j in range(num_templates):\n",
    "        num_paths.append(np.array(i[j]).shape[0])\n",
    "max_path = max(num_paths)\n",
    "\n",
    "for idx, i in enumerate(new_total_ids_list_r1):\n",
    "    for j in range(num_templates):\n",
    "        padding = np.zeros((augment, list(atoms_each_template)[j]), dtype=int)\n",
    "#         print(padding.shape)\n",
    "        for num_path in range(max_path-(np.array(i[j]).shape[0])):\n",
    "            new_total_ids_list_r1[idx][j].append(padding.tolist())\n",
    "            \n",
    "for idx, i in enumerate(new_total_ids_list_r2):\n",
    "    for j in range(num_templates):\n",
    "        padding = np.zeros((augment, list(atoms_each_template)[j]), dtype=int)\n",
    "#         print(padding.shape)\n",
    "        for num_path in range(max_path-(np.array(i[j]).shape[0])):\n",
    "            new_total_ids_list_r2[idx][j].append(padding.tolist())\n",
    "            \n",
    "\n",
    "time2 = datetime.now() \n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[[19, 16], [19, 16], [19, 16]], [[0, 0], [0, 0], [0, 0]]],\n",
       "  [[[19, 20, 18], [19, 20, 18], [19, 20, 18]],\n",
       "   [[19, 17, 19], [19, 17, 19], [19, 17, 19]]]],\n",
       " [[[[19, 19], [19, 19], [19, 19]], [[0, 0], [0, 0], [0, 0]]],\n",
       "  [[[16, 19, 19], [16, 19, 19], [16, 19, 19]],\n",
       "   [[19, 19, 16], [19, 19, 16], [19, 19, 16]]]],\n",
       " [[[[19, 19], [19, 19], [19, 19]], [[0, 0], [0, 0], [0, 0]]],\n",
       "  [[[18, 19, 16], [18, 19, 16], [18, 19, 16]],\n",
       "   [[16, 16, 19], [16, 16, 19], [16, 16, 19]]]]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_total_ids_list_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Coverted Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug\n",
    "\n",
    "# print(colored('sim_ids', 'red', attrs=['bold']))\n",
    "# for sim_ids_list in total_ids_list_r1:\n",
    "#     for sim_ids_l in sim_ids_list:\n",
    "#         for sim_ids in sim_ids_l:\n",
    "#             for sim in sim_ids:\n",
    "#                 print(sim)\n",
    "#             print()\n",
    "\n",
    "# for sim_ids_list in total_ids_list_r2:\n",
    "#     for sim_ids_l in sim_ids_list:\n",
    "#         for sim_ids in sim_ids_l:\n",
    "#             for sim in sim_ids:\n",
    "#                 print(sim)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "- l2_sim\n",
    "- model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_sim(a, b):\n",
    "    if a.dim() == 3: #if batch_size = 1\n",
    "        a = a.unsqueeze(0)\n",
    "        b = b.unsqueeze(0)\n",
    "    a = a.transpose(1,3)\n",
    "    b = b.transpose(1,3)\n",
    "    dist = torch.nn.functional.pairwise_distance(a, b)\n",
    "    sim = torch.exp(-dist)\n",
    "    sim = sim.transpose(1,2)\n",
    "    return sim  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTP_(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, batch_size, num_templates, max_path):\n",
    "        super(NTP_, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_matrix = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.template_size = num_templates\n",
    "        self.max_path = max_path\n",
    "    def calculate_sim_avg(self, r1, r2):\n",
    "        sims_list = []\n",
    "\n",
    "        for i in range(self.template_size):\n",
    "            if len(r1[i]) == 0:\n",
    "                continue\n",
    "            if len(r2[i]) == 0:\n",
    "                continue\n",
    "            lookup_tensor_r1 = torch.tensor(r1[i], dtype=torch.long).cuda()\n",
    "            lookup_tensor_r2 = torch.tensor(r2[i], dtype=torch.long).cuda()\n",
    "            \n",
    "            embed_r1 = self.embedding_matrix(lookup_tensor_r1).cuda()\n",
    "            embed_r2 = self.embedding_matrix(lookup_tensor_r2).cuda()\n",
    "\n",
    "            sims=l2_sim(embed_r1, embed_r2)\n",
    "#             print(sims.shape)\n",
    "            avg_sims = torch.squeeze(torch.mean(sims, 2, True), dim=-1)\n",
    "\n",
    "            if avg_sims.dim() == 1:\n",
    "                avg_sims = avg_sims.unsqueeze(0)\n",
    "            sims_list.append(avg_sims)\n",
    "            \n",
    "        avg_sims_ = torch.cat(sims_list, dim=0)\n",
    "\n",
    "        return avg_sims_\n",
    "        \n",
    "        \n",
    "    def forward(self, r1, r2):\n",
    "        avg_sims = self.calculate_sim_avg(r1, r2)\n",
    "        x = torch.chunk(avg_sims, self.template_size, dim=0)\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = torch.chunk(x, self.batch_size, dim=0)\n",
    "        x = list(x)\n",
    "        for i, t in enumerate(x):\n",
    "            x[i] = torch.cat(torch.chunk(t, chunks =self.template_size ,dim=1))#template\n",
    "        sims = torch.cat(x)\n",
    "        max_sims = torch.max(sims, axis=1)[0]\n",
    "        max_sims = max_sims.reshape(self.batch_size, -1)\n",
    "        min_sims = torch.min(max_sims, axis=1)[0]\n",
    "        \n",
    "        return min_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NTP_(\n",
       "  (embedding_matrix): Embedding(21, 100)\n",
       "  (loss): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(sym2id_dict)\n",
    "pos_per_batch = 1\n",
    "batch_size = pos_per_batch + (pos_per_batch*neg_per_pos)\n",
    "print(batch_size)\n",
    "embedding_size = 100\n",
    "ntp = NTP_(vocab_size, embedding_size, batch_size, num_templates, max_path)\n",
    "ntp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = []\n",
    "for i in range(pos_per_batch):\n",
    "    answer += [1]\n",
    "    for j in range(neg_per_pos):\n",
    "        answer += [0]\n",
    "answer = torch.tensor(answer, dtype=torch.float32)\n",
    "answer = answer.cuda()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "iteration :  0\n",
      "epochs:  10\n",
      "iteration :  0\n",
      "########################loss##################### :  4.370722770690918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "report_interver_epoch = 10\n",
    "report_interver_iter = 1000\n",
    "optimizer = torch.optim.Adam(ntp.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "data_size = len(total_ids_list_r1)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        print('epochs: ',epoch)\n",
    "        \n",
    "    ntp.train()\n",
    "    for i in range(0, data_size, batch_size):\n",
    "        if i%report_interver_iter == 0 :\n",
    "            print('iteration : ',int(i/batch_size))\n",
    "        optimizer.zero_grad()\n",
    "#         ntp.zero_grad()\n",
    "        real_r = []\n",
    "        template_r = []\n",
    "        \n",
    "\n",
    "\n",
    "        r1 = new_total_ids_list_r1[i:i+batch_size]\n",
    "        r2 = new_total_ids_list_r2[i:i+batch_size]\n",
    "#         pprint(r2)\n",
    "        if len(r1)<batch_size:\n",
    "            continue\n",
    "        data_length = []\n",
    "        for i in range(len(r1[0])):#templates\n",
    "            templates = []\n",
    "            for j in range(len(r1)):#batch_size\n",
    "                templates.append(torch.tensor(r1[j][i], dtype=torch.long))\n",
    "                template = torch.cat(templates)\n",
    "            real_r.append(template)\n",
    "            \n",
    "        for i in range(len(r2[0])):#templates\n",
    "            templates = []\n",
    "            for j in range(len(r2)):#batch_size\n",
    "                templates.append(torch.tensor(r2[j][i], dtype=torch.long))\n",
    "                template = torch.cat(templates)\n",
    "            template_r.append(template)        \n",
    "        if len(real_r[0]) == 0 and len(real_r[1]) == 0:\n",
    "            continue\n",
    "            \n",
    "        y_hat = ntp.forward(real_r, template_r).cuda()\n",
    "#         print(y_hat)\n",
    "#         print(answer)\n",
    "        answer = answer.cuda()\n",
    "        loss = ntp.loss(y_hat, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        print('########################loss##################### : ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.472502\n"
     ]
    }
   ],
   "source": [
    "time2 = datetime.now() \n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_match(x, emb):\n",
    "    dist = torch.torch.nn.functional.pairwise_distance(x, emb)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5255,  0.9607,  2.0689,  ...,  0.7213,  0.5457, -1.0209],\n",
      "        [ 0.1921, -0.9273, -0.0340,  ..., -1.3148,  1.4988, -0.5186],\n",
      "        [-0.8215,  0.6012,  0.1793,  ..., -0.6662,  1.5138,  0.9429],\n",
      "        ...,\n",
      "        [ 1.4940,  0.1747, -0.7029,  ...,  1.1727,  0.6480,  1.2526],\n",
      "        [-1.0773,  1.4312,  1.2800,  ...,  0.8377,  0.6742, -0.5619],\n",
      "        [ 1.1327, -0.3858,  0.9704,  ...,  0.5682, -1.1591, -0.8872]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(ntp.parameters()):\n",
    "    print(i[1])\n",
    "    embeddings = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('p0', 'X', 'Y'), ('p1', 'Y', 'X')): [[[1, 'X', 'Y'], [7, 'Y', 'X']],\n",
       "  [[2, 'X', 'Y'], [8, 'Y', 'X']],\n",
       "  [[3, 'X', 'Y'], [9, 'Y', 'X']]],\n",
       " (('p0', 'X', 'Y'),\n",
       "  ('p1', 'X', 'Z'),\n",
       "  ('p2', 'Z', 'Y')): [[[4, 'X', 'Y'],\n",
       "   [10, 'X', 'Z'],\n",
       "   [13, 'Z', 'Y']], [[5, 'X', 'Y'],\n",
       "   [11, 'X', 'Z'],\n",
       "   [14, 'Z', 'Y']], [[6, 'X', 'Y'], [12, 'X', 'Z'], [15, 'Z', 'Y']]]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_templates = {}\n",
    "ids_rule_templates = {}\n",
    "for rule_number, template in enumerate(rules):\n",
    "    result_template_key = []\n",
    "    result_template_value = []\n",
    "    result_template_values = []\n",
    "    ids_result_template_value = []\n",
    "    ids_result_template_values = []\n",
    "    for i in range(len(template)-1):\n",
    "        rule_element=('p'+ str(int(template[i][0][1])-1), template[i][1], template[i][2])       \n",
    "        result_template_key.append(rule_element)\n",
    "        rule_element = ()\n",
    "\n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            result_template_value.append([template[j][0]+'_'+str(rule_number)+'_'+str(aug), template[j][1], template[j][2]])\n",
    "        result_template_values.append(result_template_value)\n",
    "        result_template_value = []\n",
    "    rule_templates[tuple(result_template_key)] = result_template_values\n",
    "    \n",
    "    \n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            ids_result_template_value.append([sym2id_dict[template[j][0]+'_'+str(rule_number)+'_'+\n",
    "                                                           str(aug)], template[j][1], template[j][2]])\n",
    "        ids_result_template_values.append(ids_result_template_value)\n",
    "        ids_result_template_value = []\n",
    "    ids_rule_templates[tuple(result_template_key)] = ids_result_template_values\n",
    "\n",
    "ids_rule_templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((('p0', 'X', 'Y'), ('p1', 'Y', 'X')),\n",
       "   2.390730514889583e-06,\n",
       "   ['hasCitizen(X,Y)', 'nationality(Y,X)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'Y', 'X')),\n",
       "   2.2703427475789795e-06,\n",
       "   ['nationality(X,Y)', 'hasCitizen(Y,X)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'Y', 'X')),\n",
       "   1.1742851029339363e-06,\n",
       "   ['nationality(X,Y)', 'placeOfBirth(Y,X)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   9.18394675863965e-07,\n",
       "   ['hasCitizen(X,Y)', 'placeOfBirth(X,Z)', 'locatedIn(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   8.387845014112827e-07,\n",
       "   ['placeOfBirth(X,Y)', 'hasCitizen(X,Z)', 'hasCitizen(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   1.0808163324327325e-06,\n",
       "   ['nationality(X,Y)', 'hasCitizen(X,Z)', 'locatedIn(Z,Y)'])]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking_index = []\n",
    "for key, value in ids_rule_templates.items():\n",
    "    for rule in value:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "        \n",
    "masking_index\n",
    "\n",
    "total_reuslt = []\n",
    "with open(data_name+'_rule.nl', 'w') as f:\n",
    "    for key, value in ids_rule_templates.items():\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in value:\n",
    "            result = []\n",
    "            confidence_score = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                masking_index = masking_index+[element[0]]+[0]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]).cuda())\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, 1)\n",
    "                rule_result.append(id2sym_dict[top_k.indices.item()]+'('+element[1]+','+element[2]+')')\n",
    "                confidence_score.append(match[top_k.indices])\n",
    "            f.write(str(min(confidence_score).item())+'\\t')\n",
    "            head = rule_result[0]\n",
    "            body = rule_result[1:]\n",
    "            f.write(head + ' :- ' +\", \".join(body)+'\\n')  \n",
    "            result.append((key, min(confidence_score).item(), rule_result))\n",
    "            total_reuslt.append(result)\n",
    "        f.write('\\n')\n",
    "total_reuslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_name+'_rule_batch'+str(batch_size)+'_epoch'+str(epochs)+'_aug'+\n",
    "          str(augment)+'_sorted_'+str(time1)[11:13]+str(time1)[14:16]+'.nl', 'w') as file:\n",
    "    with open(data_name+'_rule.nl', 'r') as f:\n",
    "        scores = []\n",
    "        total_scores = []\n",
    "        rule = []\n",
    "        total_rules = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "\n",
    "            if '0.' not in line.split('\\t')[0]:\n",
    "                file.write(line.split('\\t')[0])\n",
    "            if '0.' in line.split('\\t')[0]:\n",
    "                count+=1\n",
    "\n",
    "                scores.append(round(float(line.split('\\t')[0]), 8))\n",
    "                rule.append(line.split('\\t')[-1])\n",
    "                if count % augment == 0:\n",
    "                    count = 0\n",
    "                    total_scores.append(scores)\n",
    "                    total_rules.append(rule)\n",
    "                    s = torch.sort(torch.tensor(scores), descending=True).values\n",
    "                    r = torch.sort(torch.tensor(scores), descending=True).indices\n",
    "\n",
    "                    for i in range(augment):\n",
    "                        file.write(str(round(s[i].item(), 8))+'\\t')\n",
    "                        file.write(rule[r[i].item()])\n",
    "                    scores = []\n",
    "                    rule = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 5\n",
    "masking_index = []\n",
    "for key, value in ids_rule_templates.items():\n",
    "    for rule in value:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "        \n",
    "masking_index\n",
    "\n",
    "total_result = []\n",
    "with open(data_name + '_rule_batch'+str(batch_size)+'_neg'+str(neg_per_pos)+'_epoch'+str(epochs)+\n",
    "          '_aug'+str(augment)+'_top'+str(top)+'_'+str(time1)[11:13]+str(time1)[14:16]+'.nl', 'w') as f:\n",
    "    for key, value in ids_rule_templates.items():\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in value:\n",
    "            result = []\n",
    "            confidence_score = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                rule_results = []\n",
    "                confidence_scores = []\n",
    "                masking_index = masking_index+[element[0]]+[0]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]).cuda())\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, top)\n",
    "#                 print(top_k)\n",
    "#                 print(top_k.indices[0].item())\n",
    "                for i in range(top):\n",
    "                    rule_results.append(id2sym_dict[top_k.indices[i].item()]+'('+element[1]+','+element[2]+')')\n",
    "                    confidence_scores.append(match[top_k.indices[i].item()].item())\n",
    "                rule_result.append(rule_results)\n",
    "                \n",
    "                \n",
    "#                     f.write(str(min(confidence_score).item())+'\\t')\n",
    "#                     head = rule_result[0]\n",
    "#                     body = rule_result[1:]\n",
    "#                     f.write(head + ' :- ' +\", \".join(body)+'\\n')  \n",
    "                confidence_score.append(confidence_scores)\n",
    "#             print(\"################\")\n",
    "#             print(rule_result)\n",
    "            confidence_score = np.array(confidence_score)\n",
    "            confidence_score = np.min(confidence_score, axis=0)\n",
    "#             print(confidence_score)\n",
    "            for i in range(top):\n",
    "                f.write(str(confidence_score[i])+'\\t')\n",
    "                for j in range(len(rule_result)):\n",
    "                    if j == 0:\n",
    "                        f.write(rule_result[j][i] + ' :- ')\n",
    "                    elif j == len(rule_result)-1:\n",
    "                        f.write(rule_result[j][i] + '\\n')\n",
    "                    else:\n",
    "                        f.write(rule_result[j][i] + ', ')\n",
    "            f.write('\\n')\n",
    "#                 result.append((key, min(confidence_score).item(), rule_result))\n",
    "#                 total_result.append(result)\n",
    "        f.write('\\n')\n",
    "# total_result\n",
    "# print(rule_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
